{"cells":[{"cell_type":"markdown","source":["# Create and use a fine tuned model"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"db7d0f43-089d-44f5-b771-da5279d95a40"},{"cell_type":"markdown","source":["## Introduction\n","\n","This notebook demonstrates a fine tuning end to end example. The notebook uploads a training file to the openai API starts a fine tuning job and then uses the fine tuned model to enforce a certain response style.  \n","\n","The jsonl training file contains a series of examples where a user provides different vegan ingredients and gpt responds with tasty vegan recipes. The recipes are inspired by asian fusion cooking and represent a tasty mix of styles to ensure the user will enjoy the resulting meal. \n","The responses also are in html markup to ensure a consistent response and reusability within different formats, such as email messages.\n","\n","This notebook covers these topics:\n","\n","1. download a training file from a public repo\n","2. upload the jsonl fine tuning data to the api\n","3. process the fine tuning data\n","4. use the fine tuned model in a chat completion\n","\n","## Prerequisites\n","\n","- [Add a lakehouse](https://aka.ms/fabric/addlakehouse) to this notebook. You will download data from a public blob, then store the data in the lakehouse resource.\n","\n","- To work with the latest chat completion functions in openai in fabric we need to upgrade the openai library. the default library might be stuck at version 0.27 and we need at least version 1. to solve this, we're going to install version 1.12 directly into the running session. Careful! doing this will restart the current running session and any variables you might have set prviously will be wiped. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"921bb1da-0677-40ae-9e3e-6e726bb652c6"},{"cell_type":"code","source":["%pip install openai==1.12.0"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":-1,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-28T10:15:27.2865356Z","session_start_time":null,"execution_start_time":"2024-02-28T10:15:48.3538834Z","execution_finish_time":"2024-02-28T10:15:48.3540335Z","parent_msg_id":"78593c55-e549-4ae7-8a07-666dd14f227c"},"text/plain":"StatementMeta(, , -1, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting openai==1.12.0\n  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from openai==1.12.0) (3.7.1)\nCollecting distro<2,>=1.7.0 (from openai==1.12.0)\n  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\nCollecting httpx<1,>=0.23.0 (from openai==1.12.0)\n  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from openai==1.12.0) (1.10.9)\nRequirement already satisfied: sniffio in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from openai==1.12.0) (1.3.0)\nRequirement already satisfied: tqdm>4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from openai==1.12.0) (4.66.1)\nCollecting typing-extensions<5,>=4.7 (from openai==1.12.0)\n  Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\nRequirement already satisfied: idna>=2.8 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.12.0) (3.4)\nRequirement already satisfied: exceptiongroup in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.12.0) (1.1.3)\nRequirement already satisfied: certifi in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.12.0) (2023.7.22)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.12.0)\n  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.12.0) (0.14.0)\nInstalling collected packages: typing-extensions, httpcore, distro, httpx, openai\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.5.0\n    Not uninstalling typing-extensions at /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages, outside environment /nfs4/pyenv-7cd1a3d1-0732-4b37-b970-30ba463a3ec0\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: openai\n    Found existing installation: openai 0.27.8\n    Not uninstalling openai at /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages, outside environment /nfs4/pyenv-7cd1a3d1-0732-4b37-b970-30ba463a3ec0\n    Can't uninstall 'openai'. No files were found to uninstall.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 2.0.0 requires sentencepiece, which is not installed.\nsentence-transformers 2.0.0 requires torchvision, which is not installed.\ndash 2.14.0 requires Flask<2.3.0,>=1.0.4, but you have flask 3.0.0 which is incompatible.\ndash 2.14.0 requires Werkzeug<2.3.0, but you have werkzeug 3.0.1 which is incompatible.\ntensorflow 2.12.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed distro-1.9.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0 typing-extensions-4.10.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nWarning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2f257706-23b1-4213-81c0-385b2c508c40"},{"cell_type":"markdown","source":["## Parameter block\n","\n","after installing the right version of openai we set the api key and any other parameters we need."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5f3ad24e-f4ca-46c0-bb2c-0a605e3d0cf4"},{"cell_type":"code","source":["APIKEY = \"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\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"7ff8c5dd-67b7-457a-9ea3-0b6069a6b120","statement_id":10,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-28T10:15:27.3683653Z","session_start_time":null,"execution_start_time":"2024-02-28T10:15:53.7921349Z","execution_finish_time":"2024-02-28T10:15:54.047939Z","parent_msg_id":"d303a1c8-3c7f-4445-ad73-dbf4292e6d8a"},"text/plain":"StatementMeta(, 7ff8c5dd-67b7-457a-9ea3-0b6069a6b120, 10, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":[]},"id":"a35ff1c6-df91-4b04-b484-6205652055c6"},{"cell_type":"code","source":["IS_CUSTOM_DATA = False  # if True, dataset has to be uploaded manually by user\n","DATA_FOLDER = \"Files/openai\"\n","DATA_FILE = \"Chef-training.jsonl\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"7ff8c5dd-67b7-457a-9ea3-0b6069a6b120","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-28T10:15:27.4815749Z","session_start_time":null,"execution_start_time":"2024-02-28T10:15:54.4954786Z","execution_finish_time":"2024-02-28T10:15:54.7495878Z","parent_msg_id":"84d76d31-4e4a-4d06-8ed7-602b588e39e7"},"text/plain":"StatementMeta(, 7ff8c5dd-67b7-457a-9ea3-0b6069a6b120, 11, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"id":"99960077-7e63-4e50-b334-ad26421bf466"},{"cell_type":"markdown","source":["## Get the data\n","Now it's time to grab the jsonl file that includes the training prompts to make our GPT Masterchef"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7a1e5d3c-7579-481e-84ba-c1a60b37d159"},{"cell_type":"code","source":["if not IS_CUSTOM_DATA:\n","    # Download demo data files into lakehouse if not exist\n","    import os, requests\n","\n","    remote_url = \"https://raw.githubusercontent.com/AllgeierSchweiz/openai-lab/main/data/Chef-training.jsonl?token=GHSAT0AAAAAACNZ4YTYYTLSNK5FEO3IWIOCZO6J5HQ\"    \n","    download_path = f\"/lakehouse/default/{DATA_FOLDER}\"\n","\n","    if not os.path.exists(\"/lakehouse/default\"):\n","        raise FileNotFoundError(\"Default lakehouse not found, please add a lakehouse and restart the session.\")\n","    os.makedirs(download_path, exist_ok=True)\n","    if not os.path.exists(f\"{download_path}/{DATA_FILE}\"):\n","        r = requests.get(f\"{remote_url}\", timeout=30)\n","        with open(f\"{download_path}/{DATA_FILE}\", \"wb\") as f:\n","            f.write(r.content)        \n","    print(\"Downloaded demo data files into lakehouse.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"7ff8c5dd-67b7-457a-9ea3-0b6069a6b120","statement_id":12,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-28T10:15:27.6750342Z","session_start_time":null,"execution_start_time":"2024-02-28T10:15:55.2181651Z","execution_finish_time":"2024-02-28T10:15:56.0145188Z","parent_msg_id":"a6660e2c-b30a-48e7-b1ad-6f01e536511c"},"text/plain":"StatementMeta(, 7ff8c5dd-67b7-457a-9ea3-0b6069a6b120, 12, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloaded demo data files into lakehouse.\n"]}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ef5abec0-7ff1-4626-ba5b-09320656210f"},{"cell_type":"code","source":["from openai import OpenAI\n","client = OpenAI(\n","   api_key=APIKEY,\n"," )"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"7ff8c5dd-67b7-457a-9ea3-0b6069a6b120","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-28T10:15:27.7942051Z","session_start_time":null,"execution_start_time":"2024-02-28T10:15:56.401704Z","execution_finish_time":"2024-02-28T10:15:57.979429Z","parent_msg_id":"8ea7af6b-cd38-4279-940f-2a612a12b665"},"text/plain":"StatementMeta(, 7ff8c5dd-67b7-457a-9ea3-0b6069a6b120, 13, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"80889f64-62d9-460e-ac9d-76711097e5f5"},{"cell_type":"markdown","source":["## Upload the trainig file to OpenAI\n","and retrieve a handle on the file. we'll need the unique file id that is created in the next step"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"59409b0a-2c1f-4875-bb5a-d29310c6b476"},{"cell_type":"code","source":["#upload the jsonl training file to the openai service\n","fo = client.files.create(\n","  file=open(\"/lakehouse/default/Files/openai/Chef-training.jsonl\", \"rb\"),\n","  purpose=\"fine-tune\"\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"7ff8c5dd-67b7-457a-9ea3-0b6069a6b120","statement_id":14,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-28T10:16:01.3267697Z","session_start_time":null,"execution_start_time":"2024-02-28T10:16:01.7125085Z","execution_finish_time":"2024-02-28T10:16:03.2103572Z","parent_msg_id":"92aeec1b-6078-4bf7-9a78-a013b1a15cc9"},"text/plain":"StatementMeta(, 7ff8c5dd-67b7-457a-9ea3-0b6069a6b120, 14, Finished, Available)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"66c844bc-99d3-4a49-af42-2658fcd4b141"},{"cell_type":"markdown","source":["## Fine tune the model\n","Now it's time to start the fine tuning. This will take about 5-10 minutes for the jsonl file we provided.\n","we keep tabs on the job by storing it in ftjob. we'll use that later to see when it finished."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7e368b49-3ce5-442f-8266-11f36503b41b"},{"cell_type":"code","source":["#create the fine tuning with the file that was just uploaded\n","ftjob = client.fine_tuning.jobs.create(\n","  training_file=fo.id, \n","  model=\"gpt-3.5-turbo\"\n",")\n","\n","print(ftjob)\n","client.fine_tuning.jobs.list_events(fine_tuning_job_id=ftjob.id, limit=10)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"7ff8c5dd-67b7-457a-9ea3-0b6069a6b120","statement_id":22,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-28T10:31:31.3797178Z","session_start_time":null,"execution_start_time":"2024-02-28T10:31:31.7841736Z","execution_finish_time":"2024-02-28T10:31:33.3119046Z","parent_msg_id":"4ab046a1-1d15-48b8-8c9f-2161664a4a4c"},"text/plain":"StatementMeta(, 7ff8c5dd-67b7-457a-9ea3-0b6069a6b120, 22, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["FineTuningJob(id='ftjob-RmEZJsi7wDRWwOCp46cAKLUV', created_at=1709116292, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-D8U9yUgZdB4bBF6lEOriShz7', result_files=[], status='validating_files', trained_tokens=None, training_file='file-xED82c2KIi27PIxdehX9N6xt', validation_file=None, user_provided_suffix=None)\n"]},{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-Ty0LxURK57pP0lt3pibA0tAn', created_at=1709116292, level='info', message='Validating training file: file-xED82c2KIi27PIxdehX9N6xt', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-P5xdyclj5yWOosI01UH0Hwf9', created_at=1709116292, level='info', message='Created fine-tuning job: ftjob-RmEZJsi7wDRWwOCp46cAKLUV', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"},"metadata":{}}],"execution_count":15,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c97bbddd-fa0e-4673-a7f2-bcf35faf0a2b"},{"cell_type":"code","source":["#get the job fresh from the api so we can check status\n","\n","ftjob = client.fine_tuning.jobs.retrieve(ftjob.id)\n","print(ftjob.status)\n","client.fine_tuning.jobs.list_events(fine_tuning_job_id=ftjob.id, limit=10)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"7ff8c5dd-67b7-457a-9ea3-0b6069a6b120","statement_id":21,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-28T10:31:22.7776839Z","session_start_time":null,"execution_start_time":"2024-02-28T10:31:23.2359806Z","execution_finish_time":"2024-02-28T10:31:24.0867323Z","parent_msg_id":"0df63188-76e9-4f85-8650-3f88d9219b28"},"text/plain":"StatementMeta(, 7ff8c5dd-67b7-457a-9ea3-0b6069a6b120, 21, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["cancelled\n"]},{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-9NYtBSbRzWloDsp1x7mFhMil', created_at=1709115418, level='info', message='Files validated, moving job to queued state', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-iE27UaJwfJadkuS80XNQxiS9', created_at=1709115396, level='info', message='Validating training file: file-xED82c2KIi27PIxdehX9N6xt', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-7eFcOuQQBwnRqWHYF9xtdueS', created_at=1709115396, level='info', message='Created fine-tuning job: ftjob-PYInWmWWE7vRAVsFIEzWFVQV', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"},"metadata":{}}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8dec8fe5-6121-41f5-999f-c7b55cb8998c"},{"cell_type":"markdown","source":["## Wait until the fine tuning job has completed\n","This can take 5-6 Minutes, but due to resource constraints this can also take several hours and be stuck in a queuing state. Might make sense to record the id of the job manually after training has completed. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"86806cc7-eb06-4ae6-973d-2cb16a8b7382"},{"cell_type":"code","source":["#use below if you lost the ftjob for any reason to get the first running job.\n","#jobs = client.fine_tuning.jobs.list(limit=1)\n","#ftjob = jobs.data[0]\n","\n","#check the job status. Wait until finished\n","import time\n","while True:\n","    sec = 60\n","    # Wait for 60 seconds\n","    time.sleep(sec)  \n","    # Retrieve the run status\n","    ftjob = client.fine_tuning.jobs.retrieve(ftjob.id)\n","\n","    run_status = ftjob.status\n","    print(f'{run_status} - {sec} seconds later...')\n","    # If run is completed, get messages\n","    if run_status == 'succeeded' or  run_status == 'cancelled':        \n","        break\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"7ff8c5dd-67b7-457a-9ea3-0b6069a6b120","statement_id":25,"state":"finished","livy_statement_state":"cancelled","queued_time":"2024-02-28T10:35:52.8473532Z","session_start_time":null,"execution_start_time":"2024-02-28T10:35:53.3045048Z","execution_finish_time":"2024-02-28T13:18:04.3493523Z","parent_msg_id":"abe2a84f-a827-4d9a-a498-edbc6ddd0209"},"text/plain":"StatementMeta(, 7ff8c5dd-67b7-457a-9ea3-0b6069a6b120, 25, Finished, Cancelled)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["queued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nqueued - 60 seconds later...\nrunning - 60 seconds later...\nrunning - 60 seconds later...\nrunning - 60 seconds later...\nrunning - 60 seconds later...\nrunning - 60 seconds later...\nrunning - 60 seconds later...\nrunning - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\nsucceeded - 60 seconds later...\n"]}],"execution_count":18,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"03e0a7da-eb4d-4d9a-8e11-ee84fcd880d9"},{"cell_type":"markdown","source":["## Use the newly trained model\n","Now it's time to use your newly created Masterchef. This one is geared towards vegan recipes with an asian fusion style, sometimes a mexican twist."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"930bcdf0-4d3c-4afe-8277-9ed50d43bbf5"},{"cell_type":"code","source":["#get the fine tuned model\n","ftjobid = ftjob.id \n","\n","#or come back later and put it here instead to carry on\n","#ftjobid = \"ftevent-ABC123\" \n","\n","ftjob = client.fine_tuning.jobs.retrieve(ftjobid)\n","ftmodel = ftjob.fine_tuned_model\n","print(f'using model {ftmodel} ...')\n","completion = client.chat.completions.create(\n","  model=ftmodel,\n","  messages=[    \n","    {\"role\": \"system\", \"content\": \"You are an Cooking Assistant specialising in vegan recipes. your cooking style is mediterranean asian fusion, similar to a mix between Jamie Oliver and Joanne Molinaro. You  will be given a set of ingredients and respond with a great tasting recipe involving those ingredients.\"},\n","    {\"role\": \"user\", \"content\": \"Cucumber, Capsicum, flour, Soy Sauce\"}\n","  ]\n",")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"7ff8c5dd-67b7-457a-9ea3-0b6069a6b120","statement_id":26,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-28T13:18:24.0580814Z","session_start_time":null,"execution_start_time":"2024-02-28T13:18:24.4873388Z","execution_finish_time":"2024-02-28T13:18:36.4982412Z","parent_msg_id":"719122f4-40f8-4b9b-a9b0-d1b9ddf0a59b"},"text/plain":"StatementMeta(, 7ff8c5dd-67b7-457a-9ea3-0b6069a6b120, 26, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ft:gpt-3.5-turbo-0613:allgeier-schweiz-ag::8xDUHcys\n"]}],"execution_count":19,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8756127a-fa2f-47ca-bd5e-b6b327a953be"},{"cell_type":"markdown","source":["The model will output the results in HTML which can easily be printed prettily in a Jupyter Notebook using the HTML function"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"826c7e63-89f8-4a21-90b1-e2f5eeebaa24"},{"cell_type":"code","source":["from IPython.core.display import HTML\n","HTML(completion.choices[0].message.content)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"7ff8c5dd-67b7-457a-9ea3-0b6069a6b120","statement_id":27,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-28T13:19:04.5348891Z","session_start_time":null,"execution_start_time":"2024-02-28T13:19:04.9894783Z","execution_finish_time":"2024-02-28T13:19:05.2221193Z","parent_msg_id":"a4e994d7-28e8-4cb7-9567-b76ab1b20120"},"text/plain":"StatementMeta(, 7ff8c5dd-67b7-457a-9ea3-0b6069a6b120, 27, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h2>Fusion Cucumber Pancakes</h2>\n<p><strong>Ingredients:</strong></p>\n<ul>\n<li>1 large cucumber, grated and squeezed dry</li>\n<li>1 capsicum, finely diced</li>\n<li>1 cup all-purpose flour</li>\n<li>1/4 cup soy sauce</li>\n<li>1/2 cup water</li>\n<li>2 tablespoons olive oil</li>\n<li>1 tablespoon sesame oil</li>\n<li>2 cloves garlic, minced</li>\n<li>1 teaspoon grated ginger</li>\n<li>Salt and pepper to taste</li>\n<li>1/4 cup chopped spring onions</li>\n<li>For Dipping:</li>\n<li>1/4 cup soy sauce</li>\n<li>1 tablespoon rice vinegar</li>\n<li>1 teaspoon honey or agave syrup</li>\n<li>Chopped spring onions and sesame seeds for garnish</li>\n</ul>\n<p><strong>Instructions:</strong></p>\n<ol>\n<li>Mix Batter: In a bowl, combine flour, soy sauce, water, olive oil, sesame oil, garlic, ginger, salt, and pepper. Whisk until smooth.</li>\n<li>Add Vegetables: Fold in grated cucumber, diced capsicum, and chopped spring onions.</li>\n<li>Cook Pancakes: Heat a non-stick pan over medium heat. Pour 1/4 cup of batter per pancake and spread to about 1/4 inch thick. Cook for 2-3 minutes on each side until golden brown.</li>\n<li>Make Dipping Sauce: Combine soy sauce, rice vinegar, and honey in a small bowl. Stir until honey dissolves.</li>\n<li>Serve: Garnish pancakes with chopped spring onions and sesame seeds. Serve with dipping sauce on the side.</li>\n</ol>"},"metadata":{}}],"execution_count":20,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{}},"id":"764b6fdf-eee7-427d-916b-b051d2dec6ab"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"default_lakehouse":"30fe4e1a-671b-4721-aac3-6a1ea26b7357","known_lakehouses":[{"id":"30fe4e1a-671b-4721-aac3-6a1ea26b7357"}],"default_lakehouse_name":"lh_FoodWaste","default_lakehouse_workspace_id":"3dc52a27-0975-4ec2-a711-94205400bada"}}},"nbformat":4,"nbformat_minor":5}