{"cells":[{"cell_type":"markdown","id":"db7d0f43-089d-44f5-b771-da5279d95a40","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Create and use a fine tuned model"]},{"cell_type":"markdown","id":"921bb1da-0677-40ae-9e3e-6e726bb652c6","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Introduction\n","\n","This notebook demonstrates a fine tuning end to end example. The notebook uploads a training file to the openai API starts a fine tuning job and then uses the fine tuned model to enforce a certain response style.  \n","\n","The jsonl training file contains a series of examples where a user provides different vegan ingredients and gpt responds with tasty vegan recipes. The recipes are inspired by asian fusion cooking and represent a tasty mix of styles to ensure the user will enjoy the resulting meal. \n","The responses also are in html markup to ensure a consistent response and reusability within different formats, such as email messages.\n","\n","This notebook covers these topics:\n","\n","1. download a training file from a public repo\n","2. upload the jsonl fine tuning data to the api\n","3. process the fine tuning data\n","4. use the fine tuned model in a chat completion\n","\n","## Prerequisites\n","\n","- [Add a lakehouse](https://aka.ms/fabric/addlakehouse) to this notebook. You will download data from a public blob, then store the data in the lakehouse resource.\n","\n","- To work with the latest chat completion functions in openai in fabric we need to upgrade the openai library. the default library might be stuck at version 0.27 and we need at least version 1. to solve this, we're going to install version 1.12 directly into the running session. Careful! doing this will restart the current running session and any variables you might have set prviously will be wiped. "]},{"cell_type":"code","execution_count":null,"id":"2f257706-23b1-4213-81c0-385b2c508c40","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%pip install openai==1.12.0"]},{"cell_type":"markdown","id":"5f3ad24e-f4ca-46c0-bb2c-0a605e3d0cf4","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Parameter block\n","\n","after installing the right version of openai we set the api key and any other parameters we need."]},{"cell_type":"code","execution_count":null,"id":"a35ff1c6-df91-4b04-b484-6205652055c6","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":true},"nteract":{"transient":{"deleting":false}},"tags":[]},"outputs":[],"source":["APIKEY = \"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\""]},{"cell_type":"code","execution_count":null,"id":"99960077-7e63-4e50-b334-ad26421bf466","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"outputs":[],"source":["IS_CUSTOM_DATA = False  # if True, dataset has to be uploaded manually by user\n","DATA_FOLDER = \"Files/openai\"\n","DATA_FILE = \"Chef-training.jsonl\""]},{"cell_type":"markdown","id":"7a1e5d3c-7579-481e-84ba-c1a60b37d159","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Get the data\n","Now it's time to grab the jsonl file that includes the training prompts to make our GPT Masterchef"]},{"cell_type":"code","execution_count":null,"id":"ef5abec0-7ff1-4626-ba5b-09320656210f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["if not IS_CUSTOM_DATA:\n","    # Download demo data files into lakehouse if not exist\n","    import os, requests\n","\n","    remote_url = \"https://raw.githubusercontent.com/AllgeierSchweiz/openai-lab/main/data/Chef-training.jsonl?token=GHSAT0AAAAAACNZ4YTYYTLSNK5FEO3IWIOCZO6J5HQ\"    \n","    download_path = f\"/lakehouse/default/{DATA_FOLDER}\"\n","\n","    if not os.path.exists(\"/lakehouse/default\"):\n","        raise FileNotFoundError(\"Default lakehouse not found, please add a lakehouse and restart the session.\")\n","    os.makedirs(download_path, exist_ok=True)\n","    if not os.path.exists(f\"{download_path}/{DATA_FILE}\"):\n","        r = requests.get(f\"{remote_url}\", timeout=30)\n","        with open(f\"{download_path}/{DATA_FILE}\", \"wb\") as f:\n","            f.write(r.content)        \n","    print(\"Downloaded demo data files into lakehouse.\")"]},{"cell_type":"code","execution_count":null,"id":"80889f64-62d9-460e-ac9d-76711097e5f5","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from openai import OpenAI\n","client = OpenAI(\n","   api_key=APIKEY,\n"," )"]},{"cell_type":"markdown","id":"59409b0a-2c1f-4875-bb5a-d29310c6b476","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Upload the trainig file to OpenAI\n","and retrieve a handle on the file. we'll need the unique file id that is created in the next step"]},{"cell_type":"code","execution_count":null,"id":"66c844bc-99d3-4a49-af42-2658fcd4b141","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#upload the jsonl training file to the openai service\n","fo = client.files.create(\n","  file=open(\"/lakehouse/default/Files/openai/Chef-training.jsonl\", \"rb\"),\n","  purpose=\"fine-tune\"\n",")"]},{"cell_type":"markdown","id":"7e368b49-3ce5-442f-8266-11f36503b41b","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Fine tune the model\n","Now it's time to start the fine tuning. This will take about 5-10 minutes for the jsonl file we provided.\n","we keep tabs on the job by storing it in ftjob. we'll use that later to see when it finished."]},{"cell_type":"code","execution_count":null,"id":"c97bbddd-fa0e-4673-a7f2-bcf35faf0a2b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#create the fine tuning with the file that was just uploaded\n","ftjob = client.fine_tuning.jobs.create(\n","  training_file=fo.id, \n","  model=\"gpt-3.5-turbo\"\n",")\n","\n","print(ftjob)\n","client.fine_tuning.jobs.list_events(fine_tuning_job_id=ftjob.id, limit=10)"]},{"cell_type":"code","execution_count":null,"id":"8dec8fe5-6121-41f5-999f-c7b55cb8998c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#get the job fresh from the api so we can check status\n","\n","ftjob = client.fine_tuning.jobs.retrieve(ftjob.id)\n","print(ftjob.status)\n","client.fine_tuning.jobs.list_events(fine_tuning_job_id=ftjob.id, limit=10)"]},{"cell_type":"markdown","id":"86806cc7-eb06-4ae6-973d-2cb16a8b7382","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Wait until the fine tuning job has completed\n","This can take 5-6 Minutes, but due to resource constraints this can also take several hours and be stuck in a queuing state. Might make sense to record the id of the job manually after training has completed. "]},{"cell_type":"code","execution_count":null,"id":"03e0a7da-eb4d-4d9a-8e11-ee84fcd880d9","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#use below if you lost the ftjob for any reason to get the first running job.\n","#jobs = client.fine_tuning.jobs.list(limit=1)\n","#ftjob = jobs.data[0]\n","\n","#check the job status. Wait until finished\n","import time\n","while True:\n","    sec = 60\n","    # Wait for 60 seconds\n","    time.sleep(sec)  \n","    # Retrieve the run status\n","    ftjob = client.fine_tuning.jobs.retrieve(ftjob.id)\n","\n","    run_status = ftjob.status\n","    print(f'{run_status} - {sec} seconds later...')\n","    # If run is completed, get messages\n","    if run_status == 'succeeded' or  run_status == 'cancelled':        \n","        break\n"]},{"cell_type":"markdown","id":"930bcdf0-4d3c-4afe-8277-9ed50d43bbf5","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Use the newly trained model\n","Now it's time to use your newly created Masterchef. This one is geared towards vegan recipes with an asian fusion style, sometimes a mexican twist."]},{"cell_type":"code","execution_count":null,"id":"8756127a-fa2f-47ca-bd5e-b6b327a953be","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#get the fine tuned model\n","ftjobid = ftjob.id \n","\n","#or come back later and put it here instead to carry on\n","#ftjobid = \"ftevent-ABC123\" \n","\n","ftjob = client.fine_tuning.jobs.retrieve(ftjobid)\n","ftmodel = ftjob.fine_tuned_model\n","print(f'using model {ftmodel} ...')\n","completion = client.chat.completions.create(\n","  model=ftmodel,\n","  messages=[    \n","    {\"role\": \"system\", \"content\": \"You are an Cooking Assistant specialising in vegan recipes. your cooking style is mediterranean asian fusion, similar to a mix between Jamie Oliver and Joanne Molinaro. You  will be given a set of ingredients and respond with a great tasting recipe involving those ingredients.\"},\n","    {\"role\": \"user\", \"content\": \"Cucumber, Capsicum, flour, Soy Sauce\"}\n","  ]\n",")\n"]},{"cell_type":"markdown","id":"826c7e63-89f8-4a21-90b1-e2f5eeebaa24","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["The model will output the results in HTML which can easily be printed prettily in a Jupyter Notebook using the HTML function"]},{"cell_type":"code","execution_count":20,"id":"764b6fdf-eee7-427d-916b-b051d2dec6ab","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-02-28T13:19:05.2221193Z","execution_start_time":"2024-02-28T13:19:04.9894783Z","livy_statement_state":"available","parent_msg_id":"a4e994d7-28e8-4cb7-9567-b76ab1b20120","queued_time":"2024-02-28T13:19:04.5348891Z","session_id":"7ff8c5dd-67b7-457a-9ea3-0b6069a6b120","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":27},"text/plain":["StatementMeta(, 7ff8c5dd-67b7-457a-9ea3-0b6069a6b120, 27, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<h2>Fusion Cucumber Pancakes</h2>\n","<p><strong>Ingredients:</strong></p>\n","<ul>\n","<li>1 large cucumber, grated and squeezed dry</li>\n","<li>1 capsicum, finely diced</li>\n","<li>1 cup all-purpose flour</li>\n","<li>1/4 cup soy sauce</li>\n","<li>1/2 cup water</li>\n","<li>2 tablespoons olive oil</li>\n","<li>1 tablespoon sesame oil</li>\n","<li>2 cloves garlic, minced</li>\n","<li>1 teaspoon grated ginger</li>\n","<li>Salt and pepper to taste</li>\n","<li>1/4 cup chopped spring onions</li>\n","<li>For Dipping:</li>\n","<li>1/4 cup soy sauce</li>\n","<li>1 tablespoon rice vinegar</li>\n","<li>1 teaspoon honey or agave syrup</li>\n","<li>Chopped spring onions and sesame seeds for garnish</li>\n","</ul>\n","<p><strong>Instructions:</strong></p>\n","<ol>\n","<li>Mix Batter: In a bowl, combine flour, soy sauce, water, olive oil, sesame oil, garlic, ginger, salt, and pepper. Whisk until smooth.</li>\n","<li>Add Vegetables: Fold in grated cucumber, diced capsicum, and chopped spring onions.</li>\n","<li>Cook Pancakes: Heat a non-stick pan over medium heat. Pour 1/4 cup of batter per pancake and spread to about 1/4 inch thick. Cook for 2-3 minutes on each side until golden brown.</li>\n","<li>Make Dipping Sauce: Combine soy sauce, rice vinegar, and honey in a small bowl. Stir until honey dissolves.</li>\n","<li>Serve: Garnish pancakes with chopped spring onions and sesame seeds. Serve with dipping sauce on the side.</li>\n","</ol>"],"text/plain":["<IPython.core.display.HTML object>"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.core.display import HTML\n","HTML(completion.choices[0].message.content)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"default_lakehouse":"30fe4e1a-671b-4721-aac3-6a1ea26b7357","default_lakehouse_name":"lh_FoodWaste","default_lakehouse_workspace_id":"3dc52a27-0975-4ec2-a711-94205400bada","known_lakehouses":[{"id":"30fe4e1a-671b-4721-aac3-6a1ea26b7357"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
